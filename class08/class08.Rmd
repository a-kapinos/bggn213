---
title: "Machine Learning 01"
author: "Andrew Kapinos"
date: "10/22/2021"
output: pdf_document
---

# Clustering methods

Kmeans clustering in R is done with the `kmeans()` function.
First we'll make up some data to test and learn with.

```{r}
tmp <- c(rnorm(30,3), rnorm(30,-3))
data <- cbind(x=tmp,y=rev(tmp))
plot(data)
```

When using Kmeans, we'll need to specify how many clusters (centers) we want.
Run `kmeans()`, setting k = 2 and nstart = 20.

```{r}
km <- kmeans(data, centers=2, nstart=20)
km
```

>Q. How many points are in each cluster?

```{r}
km$size
```
There are 30 points in each cluster.

>Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

>Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

>Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15)
```

# Hierarchical Clustering

We will use the `hclust()` function on the same data as before and see how this method works.

Unlike Kmeans, we'll need to do a little more work to determine the cluster membership when using Hclust.

```{r}
hc <- hclust(dist(data))
hc
```

Hclust has a plot method:

```{r}
plot(hc)
abline(h=7,col="red")
```

To find our membership vector, we need to "cut the tree/dendrogram; for this, we use the `cutree()` function and tell it the height to cut at.

```{r}
cutree(hc,h=7)
```

We can also use 'cutree()' and state the number of k clusters we want.

```{r}
grps <- cutree(hc,k=2)
grps
plot(data, col=grps)
```

In sum, `kmeans()` requires that we specify the data and number of centers, while `hclust()` requires that we specify the distance/dissimilarity structure of the data.


## Principal Component Analysis (PCA)

PCA is a useful analysis method when you have lots of dimensions in your data...

# PCA of UK food data

# Data Import and Checking Data

First going to import the data from the csv file

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

Complete the following code to find out how many rows and columns are in x?
___(x)

```{r}
dim(x)
```

There's only meant to be 4 col in the dataset, because there are 4 countries. What's gone wrong?

>Q. Preview the first 6 rows.

```{r}
head(x)
```

We can see that the row titles are being stored as a column. Let's fix it.

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
dim(x)
```

Much better! Let's check again to see how many rows and columns there are now.

```{r}
dim(x)
```

Great; there are 17 rows and 4 columns.

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The first approach (ie. using x <- x[,-1]) will remove data each time it is run (ie. if run again, the England column would disappear and the row names would become the values from that deleted column). We should instead just reload the data using an argument in the read.csv() function, which loads the data in as we'd like without having to manipulate the data further.

# Spotting Major Differences and Trends

Let's plot the data

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3. Changing what optional argument in the above barplot() function results in the following plot?

If we remove the beside=TRUE argument, then the bars will not be plotted besides one another. See below.

```{r}
barplot(as.matrix(x),col=rainbow(nrow(x)))
```

>Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
mycols <- rainbow(nrow(x))
pairs(x,col=mycols, pch=16)
```

The axes for each plot are determined by where the countries' names are positioned. The vertical axis for each row of plots is indicated by the country name in that row, while the horizontal axis for each column of plots is indicated by the country name in that column.

Eg. the vertical axis for the first row of plots is England, while the horizontal axis for the first column of plots is England. In the second plot of the first row (ie. plot to the right of 'England'), the axes are England v. Wales.

If the values for each country are the same, the respective point for that value should be found on the diagonal (where x=y). We can look for departures from the diagonal to identify instances in which the values in a comparison are significantly different.

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

N. Ireland seems to be the most unique, given that it participates in the most plots which exhibit values that do not fall along the diagonal (ie. has the most values that deviate significantly from the other countries). 

# PCA to the rescue

Here we will use the base R function for PCA, which is called `prcomp()`. We'll need to transpose the data using `t()` so that the prcomp() function is analyzing the proper data.

```{r}
t(x)
pca <- prcomp(t(x))
summary(pca)
```

What happens if we plot this pca data?

```{r}
plot(pca)
```

We really want to visualize something called the score plot (a.k.a. PCA plot). This is basically the plot of PCA1 v. PCA2... etc.

```{r}
attributes(pca)
```
We are after the pca$x component for this plot...

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))

# In class, a shortened version of this was used:
# plot(pca$x[,1:2])
# text(pca$x[,1:2],labels=colnames(x))
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
mycols_pca <- c("orange","red","blue","green4")
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x),col=mycols_pca)

```

vnskdfjbn



## PCA "Loadings"

We can also examine the PCA "loadings", which tell us how much the original variable contibute to each PC. Lets focus on PC1 as it accounts for > 90% of variance.

```{r}
pca$rotation
par(mar=c(10,3,2,0))
barplot(pca$rotation[,1],las=2,main="PC1 Loadings")
```

Along PC1 we can go in the positive or negative direction. Comparing this plot to the plot of PC1 v. PC2, we can observe how some observations can "push" countries to one side or the other, depending on their loadings. Eg. high negative scores, like Fresh_fruit and Alcoholic_drinks, push Wales, England, and Scotland to the left side of the plot. High positive scores, like Fresh_potatoes and Soft_drinks, push N. Ireland to the right side of the plot.

>Q9. Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 mainly tell us about?

```{r}
par(mar=c(10,3,2,0))
barplot(pca$rotation[,2],las=2,main="PC2 Loadings")
```
Fresh_potatoes and Soft_drinks feature prominently in this plot. The loading plot for PC2 tells us which groups contribute most heavily towards the remaining variance that is observed in the sample, after accounting for PC1.

## One more PCA for today

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
>Q10: How many genes and samples are in this data set?

```{r}
nrow(rna.data)
```

```{r}
ncol(rna.data)
colnames(rna.data)
```

100 genes and 10 samples.

Let's run PCA!

Using the scale argument helps us to normalize for the differences in ranges between observations.

```{r}
pca.rna <- prcomp(t(rna.data), scale=TRUE)
```

Let's make a basic plot of the data.
```{r}
plot(pca.rna$x[,1], pca.rna$x[,2], xlab="PC1", ylab="PC2")
text(pca.rna$x[,1:2], labels = colnames(rna.data))
```

```{r}
summary(pca.rna)
```

PC1 does very good at capturing the variance in the data, over 92% of variance is captured by PC1! Let's make a scree plot to visualize this.

```{r}
plot(pca.rna, main="Quick scree plot")
```

We can make our own scree plots too!
```{r}
## Variance captured per PC 
pca.var <- pca.rna$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per

barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

We can make our PCA plot a bit more useful and attractive by updating the script
```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca.rna$x[,1], pca.rna$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca.rna$x[,1], pca.rna$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

